{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74fcd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864961a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b314739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data = pd.read_csv(\"C:/Users/I_am_AlexX/Downloads/german.data\",sep=' ',names=[\"Estado_de_la_cuenta_de_cheques_existente\",\"Duración_en_meses\",\"Historial_de_crédito\",\"Propósito\",\"Monto_de_crédito\",\"Cuenta_de_ahorros\",\"Empleo_actual_desde\",\"Tasa_de_cuota_en_porcentaje_del_ingreso_disponible\",\"Estado_personal_y_sexo\",\"Otros_deudores\",\"Residencia_actual_desde\",\"Propiedad\",\"Edad_en_años\",\"Otros_planes_de_cuotas\",\"Alojamiento\",\"Número_de_créditos_existentes_en_este_banco\",\"Trabajo\",\"Número_de_personas_que_están_obligadas_a_proporcionar_alimentos_a\",\"Teléfono\",\"Trabajador_extranjero\",\"Clase\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dcd821",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca580aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cd07f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data['Clase'] = credit_data['Clase']-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd52888",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data['Clase'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6e75a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5ed873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d87c559",
   "metadata": {},
   "outputs": [],
   "source": [
    "#credit_data.groupby([\"Historial_de_crédito\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfb86cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data.groupby([\"Historial_de_crédito\"])['Clase'].agg(['count','sum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9715e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#credit_data.groupby(['Historial_de_crédito']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d651e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data.groupby(['Historial_de_crédito']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103e8401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#credit_data.groupby(['Historial_de_crédito'])[\"Clase\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee8140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#credit_data.groupby(['Historial_de_crédito'])[\"Clase\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805b0671",
   "metadata": {},
   "outputs": [],
   "source": [
    "#credit_data.groupby(['Historial_de_crédito'])[\"Clase\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7344538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El valor de la variable de clase \"1\" indica incumplimiento y \"0\" describe no incumplimiento\n",
    "\n",
    "# Si el tipo de datos es objeto, esto significa que es una variable categórica y cualquier otra variable, como int64, etc., \n",
    "# se tratará como continua y se clasificará en 10 partes iguales (también conocidas como deciles) según corresponda.\n",
    "\n",
    "def IV_calc(data,var):\n",
    "    if data[var].dtypes == \"object\":\n",
    "        \n",
    "        dataf = data.groupby([var])['Clase'].agg(['count','sum']) #.agg--> Agregue usando una o más operaciones sobre el \n",
    "        # eje especificado. \n",
    "        # Los datos se agrupan por \"var\" (la variable en cuestión) mostrando sólo la columna correspondiente a la variable\n",
    "        # \"Clase\". Y efectuando las operaciones \"count\" y \"sum\" . La primera cuenta el número de filas en el data set original \n",
    "        #que contienen alguna de las etiquetas de \"Historial_de_crédito\"(por ejemplo), estos son: \"A30,A31,A32,A33 y A34\". En cuanto a sum()\n",
    "        # suma los valores que tiene cada etiqueta ya sea \"0\" o \"1\"\n",
    "         \n",
    "        dataf.columns = [\"Total\",\"incumplido\"]\n",
    "        dataf[\"cumplido\"] = dataf[\"Total\"] - dataf[\"incumplido\"] # cuenta total de personas cumplidas\n",
    "        \n",
    "        dataf[\"per_incump\"] = dataf[\"incumplido\"]/dataf[\"incumplido\"].sum() # porcentaje del total de personas incumplidas \n",
    "                                                                            # para cada Historial de crédito específico\n",
    "        \n",
    "        dataf[\"per_cump\"] = dataf[\"cumplido\"]/dataf[\"cumplido\"].sum()       # porcentaje del total de personas cumplidas \n",
    "                                                                            # para cada Historial de crédito específico\n",
    "            \n",
    "        dataf[\"I_V\"] = (dataf[\"per_cump\"] - dataf[\"per_incump\"])*np.log(dataf[\"per_cump\"]/dataf[\"per_incump\"]) # Cálculo\n",
    "        \n",
    "        # explícito del valor de la información o \"Information Value\"\n",
    "                \n",
    "        return dataf\n",
    "    else:\n",
    "        \n",
    "        data['bin_var'] = pd.qcut(data[var].rank(method='first'),10) # qcut, Discretiza la variable en cubos de igual tamaño \n",
    "                                                                     # según el rango o según los cuantiles de muestra. \n",
    "                                                                     # Por ejemplo, 1000 valores para 10 cuantiles producirían \n",
    "                                                                     # un objeto categórico que indicaría la pertenencia al \n",
    "                                                                     #cuantil para cada punto de datos.\n",
    "        \n",
    "        \n",
    "                                                                     # rank->asigna un número según el valor que tenga en \n",
    "                                                                     #la columna del menor(0) al mayor(1000)\n",
    "            \n",
    "        dataf = data.groupby(['bin_var'])['Clase'].agg(['count','sum']) #.agg--> Agregue usando una o más operaciones sobre el \n",
    "        # eje especificado. \n",
    "        # Los datos se agrupan por \"bin_var\" (es decir por cada uno de los 10 cuantiles) mostrando sólo la columna correspondiente \n",
    "        #a la variable \"Clase\". Y efectuando las operaciones \"count\" y \"sum\" . La primera cuenta el número de filas en el data set \n",
    "        #original que contienen alguna de las etiquetas de \"bin_var\", estos son: \"(0.999, 100.9],(100.9, 200.8],(300.7, 400.6]\n",
    "        #,etc\". En cuanto a sum() suma los valores que tiene cada etiqueta ya sea \"0\" o \"1\"\n",
    "        \n",
    "        dataf.columns = [\"Total\",\"incumplido\"] #asignando headers a las columnas\n",
    "        \n",
    "        dataf[\"cumplido\"] = dataf[\"Total\"] - dataf[\"incumplido\"]  # cuenta total de personas cumplidas\n",
    "        \n",
    "        dataf[\"per_incump\"] = dataf[\"incumplido\"]/dataf[\"incumplido\"].sum() # porcentaje del total de personas incumplidas \n",
    "                                                                            # para cada valor de \"Duración en meses\" específico\n",
    "        \n",
    "        dataf[\"per_cump\"] = dataf[\"cumplido\"]/dataf[\"cumplido\"].sum() # porcentaje del total de personas incumplidas \n",
    "                                                                      # para cada valor de \"Duración en meses\" específico\n",
    "            \n",
    "        dataf[\"I_V\"] = (dataf[\"per_cump\"] - dataf[\"per_incump\"])*np.log(dataf[\"per_cump\"]/dataf[\"per_incump\"]) # Cálculo\n",
    "        \n",
    "        # explícito del valor de la información o \"Information Value\"\n",
    "                \n",
    "                \n",
    "        return dataf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c912b3a4",
   "metadata": {},
   "source": [
    "## Cálculo del Information Value para 'Historial_de_crédito'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2588ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(IV_calc(credit_data,'Historial_de_crédito'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97690579",
   "metadata": {},
   "outputs": [],
   "source": [
    "IV_calc(credit_data,'Historial_de_crédito').I_V.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32464b4",
   "metadata": {},
   "source": [
    "## Cálculo del Information Value para 'Duración_en_meses'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e5db93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(IV_calc(credit_data,'Duración_en_meses'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2539b535",
   "metadata": {},
   "outputs": [],
   "source": [
    "IV_calc(credit_data,'Duración_en_meses').I_V.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeae0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.qcut(credit_data['Duración_en_meses'].rank(method='first'),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513d3a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#credit_data['Duración_en_meses'].rank(method='first') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ceb24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#credit_data['Duración_en_meses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a681fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for seq in range(5):\n",
    "#    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2559886c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.qcut(range(5), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d1aa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#credit_data['bin_var'] = pd.qcut(credit_data['Duración_en_meses'].rank(method='first'),10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d763456",
   "metadata": {},
   "source": [
    "credit_data['bin_var'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b1b268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#credit_data.groupby(['bin_var'])['Clase'].agg(['count','sum'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dced4cb3",
   "metadata": {},
   "source": [
    "## Cálculo del Information Value automatizado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2a7778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El siguiente módulo, crea un array con las variables que deseen obtener del header\n",
    "# Data Frame \"data\"\n",
    "\n",
    "#Parametros: data , n \n",
    "\n",
    "def get_variables(data,n):\n",
    "    \n",
    "    header_arr = sorted(data) #obtener un array que contiene los elementos del header del DataFrame\n",
    "    num_variables_extracted = range(n)\n",
    "    variables = np.array(header_arr)[num_variables_extracted]\n",
    "    \n",
    "    return variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fc83c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_arr=get_variables(credit_data,21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bab722b",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b325df3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.delete(var_arr,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018290f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IV_calc_auto(data):\n",
    "    \n",
    "    arr=[]\n",
    "    \n",
    "    for var in np.delete(var_arr,1):\n",
    "        \n",
    "        IV = IV_calc(data,var)\n",
    "        arr.append(IV)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4fc703",
   "metadata": {},
   "outputs": [],
   "source": [
    "IV_calc_auto(credit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e528af",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1=[]\n",
    "arr2=[]\n",
    "for dataset in IV_calc_auto(credit_data):\n",
    "    arr1.append(dataset.I_V.sum())\n",
    "for var in np.delete(var_arr,1):\n",
    "      arr2.append(var)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302110db",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ff62dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate((arr1,arr2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3375a226",
   "metadata": {},
   "outputs": [],
   "source": [
    "IV_calc_auto(credit_data)[0].I_V.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4facbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IV_calc_auto(credit_data)[1].I_V.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df4807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'Variable': arr2, 'Valor de la información (Information Value)': arr1}\n",
    "df1 = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0f3e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb96ce44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.sort_values(by='Valor de la información (Information Value)', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c9e31a",
   "metadata": {},
   "source": [
    "# Solución alternativa al problema de automatizar el cálculo de IV (Information Value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edae7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac552ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_tot=credit_data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33602ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbb13f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_tot=np.delete(arr_tot,(20,21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce199acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_tot.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e32447",
   "metadata": {},
   "outputs": [],
   "source": [
    "#discrete_columns = ['Estado_de_la_cuenta_de_cheques_existente','Historial_de_crédito', 'Propósito','Cuenta_de_ahorros', 'Empleo_actual_desde,','Tasa_de_cuota_en_porcentaje_del_ingreso_disponible',\n",
    "#       'Estado_personal_y_sexo', 'Otros_deudores','Propiedad','Otros_planes_de_cuotas', 'Alojamiento', 'Trabajo','Teléfono', 'Trabajador_extranjero']\n",
    "\n",
    "\n",
    "#continuous_columns = ['Duración_en_meses', 'Monto_de_crédito','Tasa_de_cuota_en_porcentaje_del_ingreso_disponible',\n",
    "#'Residencia_actual_desde', 'Edad_en_años','Otros_planes_de_cuotas', 'Alojamiento','Número_de_créditos_existentes_en_este_banco',\n",
    "#'Número_de_personas_que_están_obligadas_a_proporcionar_alimentos_a']\n",
    "#discrete_columns + continuous_columns\n",
    "\n",
    "\n",
    "total_columns = credit_data.columns.values\n",
    "arr_tot=np.delete(total_columns,(20,21)).tolist()\n",
    "\n",
    "# List of IV values\n",
    "Iv_list = []\n",
    "\n",
    "for col in arr_tot:\n",
    "    assigned_data = IV_calc(data = credit_data,var = col)\n",
    "    iv_val = round(assigned_data[\"I_V\"].sum(),3)\n",
    "    dt_type = credit_data[col].dtypes\n",
    "    Iv_list.append((iv_val,col,dt_type))\n",
    "\n",
    "Iv_list = sorted(Iv_list,reverse = True)\n",
    "\n",
    "\n",
    "for i in range(len(Iv_list)):\n",
    "    \n",
    "    print (Iv_list[i][0],\",\",Iv_list[i][1],\",type =\",Iv_list[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622c521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_estche =pd.get_dummies(credit_data['Estado_de_la_cuenta_de_cheques_existente'],\n",
    "prefix='estado_cuenta_exist')\n",
    "\n",
    "dummy_hc = pd.get_dummies(credit_data['Historial_de_crédito'],\n",
    "prefix='hist_cred')\n",
    "\n",
    "dummy_cueah = pd.get_dummies(credit_data['Cuenta_de_ahorros'],\n",
    "prefix='cuent_ahorros')\n",
    "\n",
    "dummy_prop = pd.get_dummies(credit_data['Propósito'],\n",
    "prefix='propósito')\n",
    "\n",
    "dummy_property = pd.get_dummies(credit_data['Propiedad'],\n",
    "prefix='propiedad')\n",
    "\n",
    "dummy_othinstpln =pd.get_dummies(credit_data['Otros_planes_de_cuotas'],\n",
    "prefix='otros_planes_cuot')\n",
    "\n",
    "dummy_empact = pd.get_dummies(credit_data['Empleo_actual_desde'], prefix='emp_act_desde')\n",
    "\n",
    "dummy_perssx = pd.get_dummies(credit_data['Estado_personal_y_sexo'],\n",
    "prefix='estado_pers_sexo')\n",
    "\n",
    "dummy_forgnwrkr = pd.get_dummies(credit_data['Trabajador_extranjero'],\n",
    "prefix='trabajador_extranj')\n",
    "\n",
    "dummy_othdts = pd.get_dummies(credit_data['Otros_deudores'],\n",
    "prefix='otros_deud')\n",
    "\n",
    "continuous_columns = ['Duration_in_month', 'Credit_amount',\n",
    "'Installment_rate_in_percentage_of_disposable_income', 'Age_in_years',\n",
    "'Number_of_existing_credits_at_this_bank']\n",
    "\n",
    "continuous_columns = ['Duración_en_meses', 'Monto_de_crédito','Tasa_de_cuota_en_porcentaje_del_ingreso_disponible',\n",
    "                      'Edad_en_años','Número_de_créditos_existentes_en_este_banco']\n",
    "\n",
    "credit_continuous = credit_data[continuous_columns]\n",
    "\n",
    "credit_data_new = pd.concat([dummy_estche,dummy_hc,dummy_cueah\n",
    ",dummy_prop,dummy_property,dummy_othinstpln,dummy_empact\n",
    ",dummy_perssx,dummy_forgnwrkr,dummy_othdts,credit_continuous,credit_data['Clase']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf085c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data_new.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d409c397",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(\n",
    "credit_data_new.drop(['Clase'] ,axis=1),credit_data_new['Clase'],train_size\n",
    "= 0.7,random_state=42)\n",
    "#y_train = pd.DataFrame(y_train)\n",
    "#y_test = pd.DataFrame(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cd3ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_cols_extra_dummy = ['estado_cuenta_exist_A11', 'hist_cred_A30',\n",
    "'propósito_A40', 'cuent_ahorros_A61','emp_act_desde_A71','estado_pers_sexo_A91',\n",
    "'otros_deud_A101','propiedad_A121', 'otros_planes_cuot_A141','trabajador_extranj_A201']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fb8966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aquí, hemos creado la lista adicional para eliminar variables insignificantes paso a paso de forma iterativa mientras \n",
    "# trabajamos en la metodología de eliminación hacia atrás; después del final de cada iteración, seguiremos agregando la \n",
    "# variable más insignificante y multicolineal a la lista remove_cols_insig, para que esas variables se eliminen mientras \n",
    "# se entrena el modelo.\n",
    "remove_cols_insig = []\n",
    "remove_cols = list(set(remove_cols_extra_dummy+remove_cols_insig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475e6eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3977fd",
   "metadata": {},
   "source": [
    "# logistic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e5d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "logistic_model = sm.Logit(y_train, sm.add_constant(x_train.drop(\n",
    "remove_cols, axis=1))).fit()\n",
    "print (logistic_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af975897",
   "metadata": {},
   "source": [
    "## Cálculo del VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a16a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ac0860",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ef3f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train.drop(remove_cols,axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c61303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnames = x_train.drop(remove_cols,axis=1).columns #elimina la primera columna de cada categoría en x_train\n",
    "for i in np.arange(0,len(cnames)): \n",
    "    xvars = list(cnames) \n",
    "    \n",
    "yvar = xvars.pop(1)\n",
    "print(yvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e556a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"\\nFactor de inflación de varianza(Variance Inflation Factor)\")\n",
    "cnames = x_train.drop(remove_cols,axis=1).columns #elimina la primera columna de cada categoría en x_train\n",
    "for i in np.arange(0,len(cnames)): \n",
    "    xvars = list(cnames) \n",
    "    yvar = xvars.pop(i)\n",
    "#print(xvars)\n",
    "#print(((yvar)))\n",
    "    print(sm.add_constant(x_train.drop(remove_cols,axis=1)[xvars]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994c62b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.drop(remove_cols,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f9af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"\\nFactor de inflación de varianza(Variance Inflation Factor)\")\n",
    "cnames = x_train.drop(remove_cols,axis=1).columns #elimina la primera columna de cada categoría en x_train\n",
    "\n",
    "\n",
    "for i in np.arange(0,len(cnames)): # se crea un array Numpy que va desde 0 hasta a longitud de cnames (en este caso 40)\n",
    "    \n",
    "    xvars = list(cnames) #lista que contiene los nombres de las columnas ya con el filtro de \"cnames\" \n",
    "    \n",
    "    yvar = xvars.pop(i) # los 40 valores de cada columna siendo esttas las variables de respuesta\n",
    "    mod = sm.OLS(x_train.drop(remove_cols,axis=1)[yvar], sm.add_constant(x_train.drop(remove_cols,axis=1)[xvars])) #calcula\n",
    "    # los mínimos cuadrados ordinarios (OLS)\n",
    "    # model = sm.OLS(Y,X) <---- sintaxis\n",
    "    \n",
    "    # X ---> sm.add_constant(x_train.drop(remove_cols,axis=1)[xvars]) ---> 40 data sets cuya primera columna es una columna de \"unos\"\n",
    "    # agregada a cada data set que antes poseía 39 coumnas al sere extraído una columna del data set original\n",
    "    \n",
    "    # Y ---> Cada header de cada columna se usa como variable de respuesta\n",
    "    \n",
    "    res = mod.fit()\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51af0220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a64f49c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1419b40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d641525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"\\nFactor de inflación de varianza(Variance Inflation Factor)\")\n",
    "cnames = x_train.drop(remove_cols,axis=1).columns #elimina la primera columna de cada categoría en x_train\n",
    "\n",
    "for i in np.arange(0,len(cnames)): # se crea un array Numpy que va desde 0 hasta a longitud de cnames (en este caso 40)\n",
    "    \n",
    "    xvars = list(cnames) #40 listas que contiene los nombres de las columnas ya con el filtro de \"cnames\" (cada una tiene 39)\n",
    "    \n",
    "    yvar = xvars.pop(i) # los 40 valores de cada columna siendo esttas las variables de respuesta\n",
    "    mod = sm.OLS(x_train.drop(remove_cols,axis=1)[yvar], x_train.drop(remove_cols,axis=1)[xvars]) #calcula\n",
    "    # los mínimos cuadrados ordinarios (OLS)\n",
    "    # model = sm.OLS(Y,X) <---- sintaxis\n",
    "    \n",
    "    # X ---> sm.add_constant(x_train.drop(remove_cols,axis=1)[xvars]) ---> 40 data sets cuya primera columna es una columna de \"unos\"\n",
    "    # agregada a cada data set que antes poseía 39 coumnas al sere extraído una columna del data set original\n",
    "    \n",
    "    # Y ---> x_train.drop(remove_cols,axis=1)[yvar] ---> Cada header de cada columna se usa como variable de respuesta\n",
    "    \n",
    "    res = mod.fit() # Se calculan 40 regresiones con mínimos cyadrados\n",
    "    vif = 1/(1-res.rsquared)\n",
    "    print (yvar,round(vif,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59caf01",
   "metadata": {},
   "source": [
    "# Cálculo correcto del VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b457292",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"\\nFactor de inflación de varianza(Variance Inflation Factor)\")\n",
    "cnames = x_train.drop(remove_cols,axis=1).columns #elimina la primera columna de cada categoría en x_train reduciendo el data\n",
    "#set original de 50 columnas a 40 columnas\n",
    "\n",
    "for i in np.arange(0,len(cnames)): # se crea un array Numpy que va desde 0 hasta a longitud de cnames (en este caso 40)\n",
    "    \n",
    "    xvars = list(cnames) #lista que contiene los nombres de las columnas ya con el filtro de \"cnames\"\n",
    "    \n",
    "    yvar = xvars.pop(i) # los 40 valores de cada columna siendo esttas las variables de respuesta (pop(i)) extrae el elemento\n",
    "    #i-ésimo de la lista xvars\n",
    "    \n",
    "    mod = sm.OLS(x_train.drop(remove_cols,axis=1)[yvar], sm.add_constant(x_train.drop(remove_cols,axis=1)[xvars])) #calcula\n",
    "    # los mínimos cuadrados ordinarios (OLS)\n",
    "    # model = sm.OLS(Y,X) <---- sintaxis\n",
    "    \n",
    "    # X ---> sm.add_constant(x_train.drop(remove_cols,axis=1)[xvars]) ---> 40 data sets cuya primera columna es una columna de \n",
    "    # \"unos\" agregada a cada data set que antes poseía 39 coumnas al serle extraído una columna del data set original\n",
    "    \n",
    "    # Se agrega una columna de unos como una variable dependiente. La razón por la que tiene que hacer esto \n",
    "    # es porque la función de regresión asume que si no hay una columna constante, entonces desea ejecutar la regresión sin \n",
    "    #una intersección. \n",
    "    \n",
    "    # Y ---> x_train.drop(remove_cols,axis=1)[yvar] ---> Cada header de cada columna se usa como variable de respuesta\n",
    "    \n",
    "    res = mod.fit() # Se calculan 40 regresiones con mínimos cuadrados\n",
    "    vif = 1/(1-res.rsquared)\n",
    "    print (yvar,round(vif,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd36b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def calculate_vif(dataset):\n",
    "    vif=pd.DataFrame()\n",
    "    vif[\"features\"] = dataset.columns \n",
    "    vif[\"VIF_value\"]=[variance_inflation_factor(dataset.values,i) for i in range(dataset.shape[1])] \n",
    "    \n",
    "    return(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c737f5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_vif(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2524ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "duncan_prestige = sm.datasets.get_rdataset(\"Duncan\", \"carData\")\n",
    "Y = duncan_prestige.data['income']\n",
    "X = duncan_prestige.data['education']\n",
    "#X = sm.add_constant(X)\n",
    "model = sm.OLS(Y,X)\n",
    "results = model.fit()\n",
    "results.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc158da",
   "metadata": {},
   "source": [
    "# Cálculo del estadístico C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f64828",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model.predict(sm.add_constant(x_train.drop(remove_cols,axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46107801",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.add_constant(x_train.drop(remove_cols,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a77413",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def1bbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(logistic_model.predict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205c98be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.add_constant(x_train.drop(remove_cols,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7919f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model.predict(sm.add_constant(x_train.drop(remove_cols,axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee4baf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69e411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statsmodels.api as sm\n",
    "# logistic_model = sm.Logit(y_train, sm.add_constant(x_train.drop(\n",
    "# remove_cols, axis=1))).fit()\n",
    "# print (logistic_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c556698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model.predict(sm.add_constant(x_train.drop (remove_cols,axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f926d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.DataFrame(logistic_model.predict(sm.add_constant(x_train.drop (remove_cols,axis=1)))) #Usamos e conjunto de prueba \n",
    "# para prededcir el valor de la variable de respuesta o a predecir \"clase\"\n",
    "y_pred.columns = [\"probs\"]\n",
    "both = pd.concat([y_train,y_pred],axis=1) #Juntamos el valor real contra el valor predicho\n",
    "print(both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622568d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "both[['Clase','probs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf247a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#zeros['_tmpkey'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3412bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = both[['Clase','probs']][both['Clase']==0] # filtro para tomar solo los valores que tienen Clase ==0 #Data_Frame\n",
    "ones = both[['Clase','probs']][both['Clase']==1] # filtro para tomar solo los valores que tienen Clase ==1 #Data_Frame\n",
    "def df_crossjoin(df1, df2, **kwargs):\n",
    "    df1['_tmpkey'] = 1 # Agrega una nueva columna llena de \"unos\" al df1 \n",
    "    df2['_tmpkey'] = 1 # Agrega una nueva columna llena de \"unos\" al df2\n",
    "    res = pd.merge(df1, df2, on='_tmpkey', **kwargs).drop('_tmpkey',axis=1) # Parecido al comando INNER JOIN \"\" ON \"\" de SQL\n",
    "    # en donde el header de la columna '_tmpkey' se usa como elemento común para unior df1 y df2 en un solo Data Frame y con \n",
    "    # drop eliminamos ambas columnas de \"_tmpkey\" creadas previamente\n",
    "    \n",
    "    # Se multplican 209 filas por 491 filas = 102,619 filas \n",
    "    \n",
    "    res.index = pd.MultiIndex.from_product((df1.index, df2.index)) # Hace multi índices a partir del producto cartesiano\n",
    "    #de múltiples iterables. \n",
    "    \n",
    "    #print(res)\n",
    "    \n",
    "    #df1.drop('_tmpkey', axis=1, inplace=True) # no retorna ningún valor \n",
    "    #df2.drop('_tmpkey', axis=1, inplace=True) # no retorna ningún valor\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc78712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d135d761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e759351",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data = df_crossjoin(ones,zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2167f962",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb19b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#joined_data['concordant_pair'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c248bd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de804b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#joined_data['discordant_pair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff5c435",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum(joined_data['discordant_pair'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bd2282",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(sum(joined_data['discordant_pair'])*1.0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4555865",
   "metadata": {},
   "outputs": [],
   "source": [
    "(joined_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc82c49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum(joined_data['tied_pair'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e4c3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(sum(joined_data['discordant_pair'])*1.0 )/(joined_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7481c9",
   "metadata": {},
   "outputs": [],
   "source": [
    " #(sum(joined_data['concordant_pair'])*1.0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24f8c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "(joined_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a34daa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(sum(joined_data['concordant_pair'])*1.0 )/(joined_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8028ad",
   "metadata": {},
   "outputs": [],
   "source": [
    " #(sum(joined_data['discordant_pair'])*1.0 )/(joined_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601817f0",
   "metadata": {},
   "source": [
    "# Cálculo del estadístico C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98791a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data['concordant_pair'] = 0 # Se agrega una columna llamada \"concordant_pair\"\n",
    "joined_data.loc[joined_data['probs_x'] > joined_data['probs_y'], # si probs_x (predicción) > probs_y \"concordant_pair\"=1\n",
    "'concordant_pair'] =1                                            # Un par es concordante si la probabilidad contra la clase 1\n",
    "                                                                 # es mayor que la clase 0 \n",
    "\n",
    "joined_data['discordant_pair'] = 0\n",
    "joined_data.loc[joined_data['probs_x'] < joined_data['probs_y'], #discordante si la probabilidad contra la clase 1 es menor \n",
    "'discordant_pair'] =1                                            #que la clase 0\n",
    "\n",
    "joined_data['tied_pair'] = 0                                     # Si ambas probabilidades son iguales, las ponemos \n",
    "joined_data.loc[joined_data['probs_x'] ==                        # en la categoría de par empatado.\n",
    "joined_data['probs_y'],'tied_pair'] =1                           # si probs_x (predicción) == probs_y \"tied_pair\"=1\n",
    "\n",
    "p_conc = (sum(joined_data['concordant_pair'])*1.0 )/(joined_data.shape[0]) # suma el total de pares concordantes y lo vuelve \n",
    "                                                                           # decimal y se divide entre el total de pares con-\n",
    "                                                                           # cordantes para ver el porcentaje de pares que son \n",
    "                                                                           # concordantes\n",
    "\n",
    "p_disc = (sum(joined_data['discordant_pair'])*1.0 )/(joined_data.shape[0]) # suma el total de pares disconcordantes y lo vuelve \n",
    "                                                                           # decimal y se divide entre el total de pares con-\n",
    "                                                                           # cordantes para ver el porcentaje de pares que son \n",
    "                                                                           # disconcordantes\n",
    "\n",
    "\n",
    "c_statistic = 0.5 + (p_conc - p_disc)/2.0\n",
    "print (\"\\nC-statistic:\",round(c_statistic,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cb7eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "both['probs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302548c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(both['Clase'],both['probs'], #Calcula Receiver operating characteristic (ROC).\n",
    "pos_label=1) # both['Clase']--> Etiquetas binarias verdaderas. both['probs']--> Puntuaciones objetivo \n",
    "             # Si y_true está entre {-1, 1} o {0, 1}, pos_label se esablece en 1\n",
    "\n",
    "roc_auc = auc(fpr,tpr) # Calcule el área bajo la curva (AUC) utilizando la regla trapezoidal. \n",
    "plt.figure()           # Coordenadas x --> fpr ; Coordenadas y --> tpr\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=lw, label='Curva ROC (area =%0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (1-Specificity)')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Curva ROC - German Credit Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e402f0d",
   "metadata": {},
   "source": [
    "# Variables eliminadas una por una."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a10049",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data_new.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a2645a",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_cols_extra_dummy = ['estado_cuenta_exist_A11', 'hist_cred_A30',\n",
    "'propósito_A40', 'cuent_ahorros_A61','emp_act_desde_A71','estado_pers_sexo_A91',\n",
    "'otros_deud_A101','propiedad_A121', 'otros_planes_cuot_A141','trabajador_extranj_A201']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40deb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_cols_insig = ['propósito_A46', 'propósito_A45', 'propósito_A44',\n",
    "'cuent_ahorros_A63','otros_planes_cuot_A143','propiedad_A123',\n",
    "'estado_cuenta_exist_A12', 'hist_cred_A32', 'hist_cred_A33',\n",
    "'propósito_A410','propósito_A49', 'propósito_A48',\n",
    "'propiedad_A122', 'estado_pers_sexo_A92','trabajador_extranj_A202','estado_pers_sexo_A94',\n",
    "'propósito_A42','otros_deud_A102','Edad_en_años','cuent_ahorros_A64','cuent_ahorros_A62',\n",
    "'cuent_ahorros_A65','otros_deud_A103','emp_act_desde_A72','emp_act_desde_A73','emp_act_desde_A74','emp_act_desde_A75']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed6d366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aquí, hemos creado la lista adicional para eliminar variables insignificantes paso a paso de forma iterativa mientras \n",
    "# trabajamos en la metodología de eliminación hacia atrás; después del final de cada iteración, seguiremos agregando la \n",
    "# variable más insignificante y multicolineal a la lista remove_cols_insig, para que esas variables se eliminen mientras \n",
    "# se entrena el modelo.\n",
    "#remove_cols_insig = []\n",
    "remove_cols1 = list(set(remove_cols_extra_dummy+remove_cols_insig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396a9fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_cols1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6591f1ef",
   "metadata": {},
   "source": [
    "## Regresión Logística con nuevas variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99409378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "logistic_model = sm.Logit(y_train, sm.add_constant(x_train.drop(\n",
    "remove_cols1, axis=1))).fit()\n",
    "print (logistic_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6b96fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad082a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"\\nFactor de inflación de varianza(Variance Inflation Factor)\")\n",
    "cnames = x_train.drop(remove_cols1,axis=1).columns #elimina la primera columna de cada categoría en x_train reduciendo el data\n",
    "#set original de 50 columnas a 40 columnas\n",
    "\n",
    "for i in np.arange(0,len(cnames)): # se crea un array Numpy que va desde 0 hasta a longitud de cnames (en este caso 40)\n",
    "    \n",
    "    xvars = list(cnames) #lista que contiene los nombres de las columnas ya con el filtro de \"cnames\"\n",
    "    \n",
    "    yvar = xvars.pop(i) # los 40 valores de cada columna siendo esttas las variables de respuesta (pop(i)) extrae el elemento\n",
    "    #i-ésimo de la lista xvars\n",
    "    \n",
    "    mod = sm.OLS(x_train.drop(remove_cols1,axis=1)[yvar], sm.add_constant(x_train.drop(remove_cols1,axis=1)[xvars])) #calcula\n",
    "    # los mínimos cuadrados ordinarios (OLS)\n",
    "    # model = sm.OLS(Y,X) <---- sintaxis\n",
    "    \n",
    "    # X ---> sm.add_constant(x_train.drop(remove_cols,axis=1)[xvars]) ---> 40 data sets cuya primera columna es una columna de \n",
    "    # \"unos\" agregada a cada data set que antes poseía 39 coumnas al serle extraído una columna del data set original\n",
    "    \n",
    "    # Se agrega una columna de unos como una variable dependiente. La razón por la que tiene que hacer esto \n",
    "    # es porque la función de regresión asume que si no hay una columna constante, entonces desea ejecutar la regresión sin \n",
    "    #una intersección. \n",
    "    \n",
    "    # Y ---> x_train.drop(remove_cols,axis=1)[yvar] ---> Cada header de cada columna se usa como variable de respuesta\n",
    "    \n",
    "    res = mod.fit() # Se calculan 40 regresiones con mínimos cuadrados\n",
    "    vif = 1/(1-res.rsquared)\n",
    "    print (yvar,round(vif,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c1e682",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = pd.DataFrame(logistic_model.predict(sm.add_constant(x_train.drop (remove_cols1,axis=1)))) #Usamos e conjunto de prueba \n",
    "# para prededcir el valor de la variable de respuesta o a predecir \"clase\"\n",
    "y_pred1.columns = [\"probs\"]\n",
    "both = pd.concat([y_train,y_pred1],axis=1) #Juntamos el valor real contra el valor predicho\n",
    "print(both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc07558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = both[['Clase','probs']][both['Clase']==0] # filtro para tomar solo los valores que tienen Clase ==0 #Data_Frame\n",
    "ones = both[['Clase','probs']][both['Clase']==1] # filtro para tomar solo los valores que tienen Clase ==1 #Data_Frame\n",
    "def df_crossjoin(df1, df2, **kwargs):\n",
    "    df1['_tmpkey'] = 1 # Agrega una nueva columna llena de \"unos\" al df1 \n",
    "    df2['_tmpkey'] = 1 # Agrega una nueva columna llena de \"unos\" al df2\n",
    "    res = pd.merge(df1, df2, on='_tmpkey', **kwargs).drop('_tmpkey',axis=1) # Parecido al comando INNER JOIN \"\" ON \"\" de SQL\n",
    "    # en donde el header de la columna '_tmpkey' se usa como elemento común para unior df1 y df2 en un solo Data Frame y con \n",
    "    # drop eliminamos ambas columnas de \"_tmpkey\" creadas previamente\n",
    "    \n",
    "    # Se multplican 209 filas por 491 filas = 102,619 filas \n",
    "    \n",
    "    res.index = pd.MultiIndex.from_product((df1.index, df2.index)) # Hace multi índices a partir del producto cartesiano\n",
    "    #de múltiples iterables. \n",
    "    \n",
    "    #print(res)\n",
    "    \n",
    "    #df1.drop('_tmpkey', axis=1, inplace=True) # no retorna ningún valor \n",
    "    #df2.drop('_tmpkey', axis=1, inplace=True) # no retorna ningún valor\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4937c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data1 = df_crossjoin(ones,zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f4e5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e9d186",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data1['concordant_pair'] = 0 # Se agrega una columna llamada \"concordant_pair\"\n",
    "joined_data1.loc[joined_data1['probs_x'] > joined_data1['probs_y'], # si probs_x (predicción) > probs_y \"concordant_pair\"=1\n",
    "'concordant_pair'] =1                                            # Un par es concordante si la probabilidad contra la clase 1\n",
    "                                                                 # es mayor que la clase 0 \n",
    "\n",
    "joined_data1['discordant_pair'] = 0\n",
    "joined_data1.loc[joined_data1['probs_x'] < joined_data1['probs_y'], #discordante si la probabilidad contra la clase 1 es menor \n",
    "'discordant_pair'] =1                                            #que la clase 0\n",
    "\n",
    "joined_data1['tied_pair'] = 0                                     # Si ambas probabilidades son iguales, las ponemos \n",
    "joined_data1.loc[joined_data1['probs_x'] ==                        # en la categoría de par empatado.\n",
    "joined_data1['probs_y'],'tied_pair'] =1                           # si probs_x (predicción) == probs_y \"tied_pair\"=1\n",
    "\n",
    "p_conc = (sum(joined_data1['concordant_pair'])*1.0 )/(joined_data1.shape[0]) # suma el total de pares concordantes y lo vuelve \n",
    "                                                                           # decimal y se divide entre el total de pares con-\n",
    "                                                                           # cordantes para ver el porcentaje de pares que son \n",
    "                                                                           # concordantes\n",
    "\n",
    "p_disc = (sum(joined_data1['discordant_pair'])*1.0 )/(joined_data1.shape[0]) # suma el total de pares disconcordantes y lo vuelve \n",
    "                                                                           # decimal y se divide entre el total de pares con-\n",
    "                                                                           # cordantes para ver el porcentaje de pares que son \n",
    "                                                                           # disconcordantes\n",
    "\n",
    "\n",
    "c_statistic = 0.5 + (p_conc - p_disc)/2.0\n",
    "print (\"\\nC-statistic:\",round(c_statistic,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6f845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be84d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287f620e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(both['Clase'],both['probs'],\n",
    "pos_label=1)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=lw, label='Curva ROC (area =%0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (1-Specificity)')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Curva ROC - German Credit Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac1633c",
   "metadata": {},
   "source": [
    "# Falta analizar el código de abajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a0e40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9d3518",
   "metadata": {},
   "outputs": [],
   "source": [
    "both[\"y_pred\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6568289",
   "metadata": {},
   "outputs": [],
   "source": [
    "both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7327da23",
   "metadata": {},
   "outputs": [],
   "source": [
    "both.loc[both[\"probs\"]>0.1,'y_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60b86fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de96f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una vez que hemos encontrado la mejor situación del conjunto de datos de entrenamiento, la siguiente y última tarea \n",
    "# es predecir la categoría desde la probabilidad hasta el valor predeterminado. Hay muchas formas de establecer el valor de \n",
    "# umbral para convertir la probabilidad predicha en una clase real. En el siguiente código, hemos realizado una búsqueda de \n",
    "# cuadrícula simple para determinar el límite de umbral de mejor probabilidad. No obstante, incluso las curvas de sensibilidad \n",
    "# y especificidad podrían utilizarse para esta tarea.\n",
    "\n",
    "\n",
    "\n",
    "for i in list(np.arange(0,1,0.1)): # crear array de 0 a 1 en pasos de 0.1\n",
    "    both[\"y_pred\"] = 0 # agregar columna \"y_pred\" con valores cero\n",
    "    both.loc[both[\"probs\"] > i, 'y_pred'] = 1 # Si e lvalor de la columna probs es mayor que los vcalores i\n",
    "                                            # se le asigna valor de 1 \n",
    "    print (\"Threshold\",i,\"Train Accuracy:\",\n",
    "round(accuracy_score(both['Clase'], both['y_pred']),4)) # Se asigna la precisión \n",
    "\n",
    "# accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f10a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora, se aplicará un umbral de 0,5 a los datos de prueba para verificar si el modelo es consistente en varios conjuntos\n",
    "# de datos con el siguiente código:\n",
    "both[\"y_pred\"] = 0\n",
    "both.loc[both[\"probs\"] > 0.5, 'y_pred'] = 1\n",
    "print (\"\\nTrain Confusion Matrix\\n\\n\", pd.crosstab(both['Clase'],\n",
    "both['y_pred'],rownames = [\"Actuall\"],colnames = [\"Predicted\"]))\n",
    "\n",
    "print (\"\\nTrain Accuracy:\",round(accuracy_score(both['Clase'],both['y_pred']),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc0cf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = pd.DataFrame( logistic_model.predict(sm.add_constant(x_test.drop(remove_cols1,axis=1)))) #Data frame con predicciones\n",
    "y_pred_test.columns = [\"probs\"] # Se le asigna el nombre \"probs\" a la columna de predicciones\n",
    "\n",
    "both_test = pd.concat([y_test,y_pred_test],axis=1) # se unen los daros reales y predichos del conjunto de prueba en un sollo DataFrame\n",
    "\n",
    "both_test[\"y_pred\"] = 0 # Se asigna una columna con ceros (300 ceros en total)}\n",
    "\n",
    "both_test.loc[both_test[\"probs\"] > 0.5, 'y_pred'] = 1 # Se filtran datos para asignar 0 o 1 según el umbral 0.5\n",
    "\n",
    "print (\"\\nPrueba de la Matriz de Confusión\\n\\n\", pd.crosstab( both_test['Clase'],\n",
    "both_test['y_pred'],rownames = [\"Real\"],colnames = [\"Predicho\"]))\n",
    "\n",
    "print (\"\\nPrecisión de la prueba(Test Accuracy):\", round(accuracy_score( both_test['Clase'],\n",
    "both_test['y_pred']),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8000736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
